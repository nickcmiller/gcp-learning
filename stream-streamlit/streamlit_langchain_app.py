import os
import logging
import asyncio
from typing import AsyncGenerator
import streamlit as st
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage, SystemMessage, AIMessage

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

groq_api_key = os.getenv('GROQ_API_KEY')
if not groq_api_key:
    logger.error("Groq API Key is not set. Please set the API key in the environment variables.")

async def generate_response(input_text: str, chat_history: list) -> AsyncGenerator[str, None]:
    """
    Generates a response using the ChatGroq model.

    Args:
        input_text (str): The user's input text.
        chat_history (list): The chat history containing previous messages.

    Yields:
        str: The generated response tokens.

    Returns:
        AsyncGenerator: An asynchronous generator that yields the response tokens.
    """
    llm = ChatGroq(
        model_name="llama3-70b-8192",
        temperature=0.2,
        groq_api_key=groq_api_key,
        streaming=True,
    )

    messages = []

    # Iterate over each message in the chat history and create a corresponding message object
    for message in chat_history:
        if message["role"] == "user":
            # Create a HumanMessage object for the user's message.
            # The HumanMessage class represents a message sent by the user in the chat.
            # It takes the content of the message as an argument.
            human_message = HumanMessage(content=message["content"])
            messages.append(human_message)
            logger.info(f"Appended HumanMessage: {human_message}")
        elif message["role"] == "assistant":
            # Create an AIMessage object for the assistant's message.
            # The AIMessage class represents a message generated by the AI assistant in the chat.
            # It takes the content of the message as an argument.
            ai_message = AIMessage(content=message["content"])
            messages.append(ai_message)
            logger.info(f"Appended AIMessage: {ai_message}")
        elif message["role"] == "system":
            # Create a SystemMessage object for the system message.
            # The SystemMessage class represents a system-generated message in the chat.
            # These messages are typically used for instructions or notifications.
            # It takes the content of the message as an argument.
            system_message = SystemMessage(content=message["content"])
            messages.append(system_message)
            logger.info(f"Appended SystemMessage: {system_message}")
    
    # After processing all messages in the chat history, append a new HumanMessage with the input text
    messages.append(HumanMessage(content=input_text))

    try:
        # Call the astream method of the ChatGroq model to generate a response based on the input text and chat history
        # The astream method returns an asynchronous generator that yields response tokens
        response = llm.astream(messages)
    except Exception as e:
        logger.error("Error during response generation: %s", e, exc_info=True)
        yield "Error generating response."

    # Iterate over the response tokens and yield each token
    async for token in response:
        yield token.content

async def generate_and_display_response(prompt: str, messages: list) -> str:
    """
    Async Streamlit function that generates and displays a response based on the given prompt and messages.

    Args:
        prompt (str): The prompt for generating the response.
        messages (list): A list of messages exchanged between the user and the assistant.

    Returns:
        str: The generated response.

    """

    # Create an empty Streamlit container to display the assistant's message
    assistant_message_container = st.empty()

    response = ""
    assistant_message = ""
    
    # Create a new container within the assistant_message_container
    with assistant_message_container.container():
        
        # Call the generate_response function with the user's prompt and the chat history
        # This function returns an asynchronous generator that yields the assistant's response
        async_gen = generate_response(prompt, messages)
        
        # Iterate over the tokens generated by the async generator
        async for token in async_gen:
            # Add the current token to the full response
            response += token
            
            # Add the current token to the assistant's message
            assistant_message += token
            
            # Display the assistant's message in the Streamlit container
            # The message is updated with each new token, creating a typing effect
            assistant_message_container.chat_message("assistant").markdown(assistant_message)
    
    st.session_state.messages.append({"role": "assistant", "content": assistant_message})
    
    return response

st.title("Groq Chat")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "You are a helpful assistant."}]

# Iterate over each message in the session state's messages
for message in st.session_state.messages:
    # Check if the role of the message is not 'system'
    # 'system' messages are typically instructions or notifications, and we don't want to display them in the chat
    if message["role"] != "system":
        # Create a new chat message with the role of the current message
        # The 'with' statement is used here to apply a context to the chat message
        # The context is the role of the message, which indicates who sent the message ('user' or 'assistant')
        with st.chat_message(message["role"]):
            # Display the content of the message in the chat using Markdown formatting
            st.markdown(message["content"])

if "current_prompt" not in st.session_state:
    st.session_state.current_prompt = "Ask me anything..."

# Check if the user has entered a prompt in the chat input field.
# The ':=' operator is known as the 'walrus operator' and is used to assign values to variables as part of an expression.
# If the user has entered a prompt, the 'if' statement will evaluate to True and the prompt will be assigned to the 'prompt' variable.
if prompt := st.chat_input(st.session_state.current_prompt):
    
    # Append the user's message to the 'messages' list in the session state.
    # Each message is represented as a dictionary with 'role' and 'content' keys.
    # The 'role' key indicates who sent the message ('user' in this case) and the 'content' key contains the text of the message.
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Create a new chat message from the user with the text of the prompt.
    # The 'with' statement is used here to apply a context to the chat message.
    # In this case, the context is 'user', which indicates that the message is from the user.
    with st.chat_message("user"):
        # Display the user's message in the chat using Markdown formatting.
        st.markdown(prompt)

    # Run the 'generate_and_display_response' function to generate a response from the AI assistant and display it in the chat.
    # The 'asyncio.run' function is used to run the 'generate_and_display_response' function, which is an asynchronous function.
    # The 'generate_and_display_response' function takes the user's prompt and the chat history as arguments.
    asyncio.run(generate_and_display_response(prompt, st.session_state.messages))

    # Update the current prompt in the session state to prompt the user to ask a follow-up question.
    if st.session_state.current_prompt == "Ask me anything...":
        st.session_state.current_prompt = "Ask a follow-up question..."
        st.rerun()